most_occuring <- most_occuring  %>%
filter(n >= 4)
most_occuring
most_occuring <- most_occuring  %>%
filter(n >= 6)
most_occuring
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = TRUE)[1:10])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
week1Tokens <-
week1_df %>%
unnest_tokens(output = word, input = Content)
install.packages("tm")
install.packages("textdata")
library(tidyverse)
library(here)
library(tm)
library(dplyr)
library(igraph)
library(tidytext)
library(dataedu)
file_path1 <- "week1.txt"
file_path2 <- "week2.txt"
file_path3 <- "week3.txt"
file_path4 <- "week4.txt"
file_path5 <- "week5.txt"
file_path6 <- "week6.txt"
text_lines1 <- readLines(file_path1, warn = FALSE)
text_lines2 <- readLines(file_path2, warn = FALSE)
text_lines3 <- readLines(file_path3, warn = FALSE)
text_lines4 <- readLines(file_path4, warn = FALSE)
text_lines5 <- readLines(file_path5, warn = FALSE)
text_lines6 <- readLines(file_path6, warn = FALSE)
week1_df <- data.frame(Content = text_lines1)
week2_df <- data.frame(Content = text_lines2)
week3_df <- data.frame(Content = text_lines3)
week4_df <- data.frame(Content = text_lines4)
week5_df <- data.frame(Content = text_lines5)
week6_df <- data.frame(Content = text_lines6)
week1Tokens <-
week1_df %>%
unnest_tokens(output = word, input = Content)
week2Tokens <-
week2_df %>%
unnest_tokens(output = word, input = Content)
week3Tokens <-
week3_df %>%
unnest_tokens(output = word, input = Content)
week4Tokens <-
week4_df %>%
unnest_tokens(output = word, input = Content)
week5Tokens <-
week5_df %>%
unnest_tokens(output = word, input = Content)
week6Tokens <-
week6_df %>%
unnest_tokens(output = word, input = Content)
View(week2_df)
View(week2_df)
week1TokensMost %>%
count(word, sort = TRUE)
week1Most <- week1Tokens %>%
count(word, sort = TRUE)
View(week1Most)
View(week1Most)
data(stop_words)
week1Tokens <-
week1Tokens %>%
anti_join(stop_words, by = "word")
week2Tokens <-
week2Tokens %>%
anti_join(stop_words, by = "word")
week3Tokens <-
week3Tokens %>%
anti_join(stop_words, by = "word")
week4Tokens <-
week4Tokens %>%
anti_join(stop_words, by = "word")
week5Tokens <-
week5Tokens %>%
anti_join(stop_words, by = "word")
week6Tokens <-
week6Tokens %>%
anti_join(stop_words, by = "word")
week1Most <- week1Tokens %>%
count(word, sort = TRUE)
View(week1Most)
View(week1Most)
week1Most <- week1Tokens %>%
%>%
week1Most <- week1Tokens %>%
%>%
week1Most <- week1Tokens %>%
filter(n >= 6)
week1Most <- week1Tokens %>%
filter(n >= 3)
week1Most <- week1Tokens %>%
filter(n >= 6)   %>%
count(word, sort = TRUE)
week1Most <- week1Tokens %>%
count(word, sort = TRUE)  %>%
filter(n >= 6)
View(week1Most)
View(week1Most)
week1Most <- week1Tokens %>%
count(word, sort = TRUE)  %>%
filter(n >= 3)
View(week1Most)
week1Most <- week1Tokens %>%
count(word, sort = TRUE)  %>%
filter(n >= 1)
View(week1Most)
View(week1Most)
data(stop_words)
week1Tokens <-
week1Tokens %>%
anti_join(stop_words, by = "word")
week2Tokens <-
week2Tokens %>%
anti_join(stop_words, by = "word")
week3Tokens <-
week3Tokens %>%
anti_join(stop_words, by = "word")
week4Tokens <-
week4Tokens %>%
anti_join(stop_words, by = "word")
week5Tokens <-
week5Tokens %>%
anti_join(stop_words, by = "word")
week6Tokens <-
week6Tokens %>%
anti_join(stop_words, by = "word")
combined_tokens <- bind_rows(week1Tokens, week2Tokens, week3Tokens, week4Tokens, week5Tokens, week6Tokens)
combined_tokens %>%
count(word, sort = TRUE)
glimpse(combined_tokens)
most_occuring <- most_occuring  %>%
filter(n >= 6)
most_occuring
most_occuring <- most_occuring  %>%
filter(n >= 5)
most_occuring
most_occuring <- most_occuring  %>%
filter(n >= 4)
most_occuring
most_occuring <- combined_tokens %>%
count(word, sort = TRUE) %>%
# n as a percent of total words
mutate(count = n)
most_occuring <- most_occuring  %>%
filter(n >= 4)
most_occuring
most_occuring <- most_occuring  %>%
filter(n >= 4)
most_occuring
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = TRUE)[1:10])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = TRUE)[1:10])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = TRUE)[1:10])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = FALSE)[1:10])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
most_occuring <- most_occuring  %>%
filter(n >= 4)
most_occuring
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = FALSE)[1:10])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = FALSE)[1:11])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = FALSE)[1:11])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.kamada.kawai)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = FALSE)[1:11])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
library(tidyverse)
library(here)
library(tm)
library(dplyr)
library(igraph)
library(tidytext)
library(dataedu)
file_path1 <- "week1.txt"
file_path2 <- "week2.txt"
file_path3 <- "week3.txt"
file_path4 <- "week4.txt"
file_path5 <- "week5.txt"
file_path6 <- "week6.txt"
text_lines1 <- readLines(file_path1, warn = FALSE)
text_lines2 <- readLines(file_path2, warn = FALSE)
text_lines3 <- readLines(file_path3, warn = FALSE)
text_lines4 <- readLines(file_path4, warn = FALSE)
text_lines5 <- readLines(file_path5, warn = FALSE)
text_lines6 <- readLines(file_path6, warn = FALSE)
week1_df <- data.frame(Content = text_lines1)
week2_df <- data.frame(Content = text_lines2)
week3_df <- data.frame(Content = text_lines3)
week4_df <- data.frame(Content = text_lines4)
week5_df <- data.frame(Content = text_lines5)
week6_df <- data.frame(Content = text_lines6)
week1Tokens <-
week1_df %>%
unnest_tokens(output = word, input = Content)
week2Tokens <-
week2_df %>%
unnest_tokens(output = word, input = Content)
week3Tokens <-
week3_df %>%
unnest_tokens(output = word, input = Content)
week4Tokens <-
week4_df %>%
unnest_tokens(output = word, input = Content)
week5Tokens <-
week5_df %>%
unnest_tokens(output = word, input = Content)
week6Tokens <-
week6_df %>%
unnest_tokens(output = word, input = Content)
data(stop_words)
week1Tokens <-
week1Tokens %>%
anti_join(stop_words, by = "word")
week2Tokens <-
week2Tokens %>%
anti_join(stop_words, by = "word")
week3Tokens <-
week3Tokens %>%
anti_join(stop_words, by = "word")
week4Tokens <-
week4Tokens %>%
anti_join(stop_words, by = "word")
week5Tokens <-
week5Tokens %>%
anti_join(stop_words, by = "word")
week6Tokens <-
week6Tokens %>%
anti_join(stop_words, by = "word")
combined_tokens <- bind_rows(week1Tokens, week2Tokens, week3Tokens, week4Tokens, week5Tokens, week6Tokens)
combined_tokens %>%
count(word, sort = TRUE)
glimpse(combined_tokens)
most_occuring <- combined_tokens %>%
count(word, sort = TRUE) %>%
# n as a percent of total words
mutate(count = n)
most_occuring <- most_occuring  %>%
filter(n >= 4)
most_occuring
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = FALSE)[1:11])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
set.seed(200)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = FALSE)[1:11])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
set.seed(201)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = TRUE)[1:20])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
set.seed(201)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = TRUE)[1:25])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
set.seed(201)
# Create a corpus
corpus <- Corpus(VectorSource(most_occuring))
# Preprocess the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
# Create a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Convert DTM to a matrix
mat <- as.matrix(dtm)
# Calculate the importance of each term (e.g., simple word counts)
term_freq <- colSums(mat)
# Select the most important terms (e.g., top 10 terms)
top_terms <- names(sort(term_freq, decreasing = TRUE)[1:25])
# Create a graph
edges <- combn(top_terms, 2)
# Create a graph from the edges
graph <- graph_from_edgelist(t(edges), directed = TRUE)
# Plot the graph
plot(graph, layout = layout.fruchterman.reingold)
