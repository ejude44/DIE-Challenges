anti_join(stop_words, by = "word")
week2Tokens <-
week2Tokens %>%
anti_join(stop_words, by = "word")
week3Tokens <-
week3Tokens %>%
anti_join(stop_words, by = "word")
week4Tokens <-
week4Tokens %>%
anti_join(stop_words, by = "word")
week5Tokens <-
week5Tokens %>%
anti_join(stop_words, by = "word")
week6Tokens <-
week6Tokens %>%
anti_join(stop_words, by = "word")
combined_tokens <- bind_rows(week1Tokens, week2Tokens, week3Tokens, week4Tokens, week5Tokens, week6Tokens)
create_word_cloud <- function(tokens){
wordcloud(words = week1Tokens, min.freq = 1, scale = c(3, 0.5), colors = brewer.pal(8, "Dark2"))
}
word_cloud <- lapply(paste0("week",1:6,"Tokens"), create_word_cloud)
combined_tokens %>%
count(word, sort = TRUE)
most_occuring <- combined_tokens %>%
count(word, sort = TRUE) %>%
# n as a percent of total words
mutate(percent = n / sum(n) * 100)
get_sentiments("nrc")
# Only positive in the NRC dataset
nrc_pos <-
get_sentiments("nrc") %>%
filter(sentiment == "positive")
# Match to tokens
pos_tokens_count <-
combined_tokens %>%
inner_join(nrc_pos, by = "word") %>%
# Total appearance of positive words
count(word, sort = TRUE)
pos_tokens_count
pos_tokens_count %>%
# only words that appear 75 times or more
filter(n >= 3) %>%
ggplot(., aes(x = reorder(word, -n), y = n)) +
geom_bar(stat = "identity", fill = dataedu_colors("darkblue")) +
labs(
title = "Count of Words Associated with Positivity",
subtitle = "Words associated with postivity",
caption = "Data: Journal and NRC",
x = "",
y = "Count"
) +
theme_dataedu()
read_and_preprocess <- function(file_path) {
text <- tolower(readLines(file_path, warn = FALSE))
text <- gsub("[[:punct:]]", " ", text)
return(text)
}
texts <- lapply(paste0("week", 1:6, ".txt"), read_and_preprocess)
positive_words <- get_sentiments("afinn")$word
# Function to count positive words in a text
count_positive_words <- function(text) {
words <- unlist(strsplit(text, " "))
positive_count <- sum(words %in% positive_words)
return(positive_count)
}
positive_counts <- sapply(texts, count_positive_words)
weeks <- paste0("Week", 1:6)
df2 <- data.frame(Week = weeks, PositiveWordsCount = positive_counts)
ggplot(df, aes(x = Week, y = PositiveWordsCount)) +
geom_line(color = "blue",size = 2) +
geom_point(color = "red", size = 3) +
geom_smooth(method = "loess", se = FALSE, color = "green") +
labs(
title = "Count of Positive Words Over Time",
x = "Week",
y = "Positive Words Count",
caption = "Smoothed trend line"
) +
theme_dataedu() +
theme(
plot.title = element_text(hjust = 0.5),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
View(most_occuring)
file_path1 <- "week1.txt"
file_path2 <- "week2.txt"
file_path3 <- "week3.txt"
file_path4 <- "week4.txt"
file_path5 <- "week5.txt"
file_path6 <- "week6.txt"
text_lines1 <- readLines(file_path1, warn = FALSE)
text_lines2 <- readLines(file_path2, warn = FALSE)
text_lines3 <- readLines(file_path3, warn = FALSE)
text_lines4 <- readLines(file_path4, warn = FALSE)
text_lines5 <- readLines(file_path5, warn = FALSE)
text_lines6 <- readLines(file_path6, warn = FALSE)
week1_df <- data.frame(Content = text_lines1)
week2_df <- data.frame(Content = text_lines2)
week3_df <- data.frame(Content = text_lines3)
week4_df <- data.frame(Content = text_lines4)
week5_df <- data.frame(Content = text_lines5)
week6_df <- data.frame(Content = text_lines6)
week1Tokens <-
week1_df %>%
unnest_tokens(output = word, input = Content)
install.packages("tm")
install.packages("wordcloud")
install.packages("textdata")
library(tidyverse)
library(here)
library(tm)
library(dplyr)
library(wordcloud)
library(tidytext)
library(dataedu)
library(ggplot2)
file_path1 <- "week1.txt"
file_path2 <- "week2.txt"
file_path3 <- "week3.txt"
file_path4 <- "week4.txt"
file_path5 <- "week5.txt"
file_path6 <- "week6.txt"
text_lines1 <- readLines(file_path1, warn = FALSE)
text_lines2 <- readLines(file_path2, warn = FALSE)
text_lines3 <- readLines(file_path3, warn = FALSE)
text_lines4 <- readLines(file_path4, warn = FALSE)
text_lines5 <- readLines(file_path5, warn = FALSE)
text_lines6 <- readLines(file_path6, warn = FALSE)
week1_df <- data.frame(Content = text_lines1)
week2_df <- data.frame(Content = text_lines2)
week3_df <- data.frame(Content = text_lines3)
week4_df <- data.frame(Content = text_lines4)
week5_df <- data.frame(Content = text_lines5)
week6_df <- data.frame(Content = text_lines6)
week1Tokens <-
week1_df %>%
unnest_tokens(output = word, input = Content)
week2Tokens <-
week2_df %>%
unnest_tokens(output = word, input = Content)
week3Tokens <-
week3_df %>%
unnest_tokens(output = word, input = Content)
week4Tokens <-
week4_df %>%
unnest_tokens(output = word, input = Content)
week5Tokens <-
week5_df %>%
unnest_tokens(output = word, input = Content)
week6Tokens <-
week6_df %>%
unnest_tokens(output = word, input = Content)
data(stop_words)
week1Tokens <-
week1Tokens %>%
anti_join(stop_words, by = "word")
week2Tokens <-
week2Tokens %>%
anti_join(stop_words, by = "word")
week3Tokens <-
week3Tokens %>%
anti_join(stop_words, by = "word")
week4Tokens <-
week4Tokens %>%
anti_join(stop_words, by = "word")
week5Tokens <-
week5Tokens %>%
anti_join(stop_words, by = "word")
week6Tokens <-
week6Tokens %>%
anti_join(stop_words, by = "word")
combined_tokens <- bind_rows(week1Tokens, week2Tokens, week3Tokens, week4Tokens, week5Tokens, week6Tokens)
create_word_cloud <- function(tokens){
wordcloud(words = week1Tokens, min.freq = 1, scale = c(3, 0.5), colors = brewer.pal(8, "Dark2"))
}
word_cloud <- lapply(paste0("week",1:6,"Tokens"), create_word_cloud)
combined_tokens %>%
count(word, sort = TRUE)
most_occuring <- combined_tokens %>%
count(word, sort = TRUE) %>%
# n as a percent of total words
mutate(percent = n / sum(n) * 100)
get_sentiments("nrc")
# Only positive in the NRC dataset
nrc_pos <-
get_sentiments("nrc") %>%
filter(sentiment == "positive")
# Match to tokens
pos_tokens_count <-
combined_tokens %>%
inner_join(nrc_pos, by = "word") %>%
# Total appearance of positive words
count(word, sort = TRUE)
pos_tokens_count
pos_tokens_count %>%
# only words that appear 75 times or more
filter(n >= 3) %>%
ggplot(., aes(x = reorder(word, -n), y = n)) +
geom_bar(stat = "identity", fill = dataedu_colors("darkblue")) +
labs(
title = "Count of Words Associated with Positivity",
subtitle = "Words associated with postivity",
caption = "Data: Journal and NRC",
x = "",
y = "Count"
) +
theme_dataedu()
read_and_preprocess <- function(file_path) {
text <- tolower(readLines(file_path, warn = FALSE))
text <- gsub("[[:punct:]]", " ", text)
return(text)
}
texts <- lapply(paste0("week", 1:6, ".txt"), read_and_preprocess)
positive_words <- get_sentiments("afinn")$word
# Function to count positive words in a text
count_positive_words <- function(text) {
words <- unlist(strsplit(text, " "))
positive_count <- sum(words %in% positive_words)
return(positive_count)
}
positive_counts <- sapply(texts, count_positive_words)
weeks <- paste0("Week", 1:6)
df2 <- data.frame(Week = weeks, PositiveWordsCount = positive_counts)
ggplot(df, aes(x = Week, y = PositiveWordsCount)) +
geom_line(color = "blue",size = 2) +
geom_point(color = "red", size = 3) +
geom_smooth(method = "loess", se = FALSE, color = "green") +
labs(
title = "Count of Positive Words Over Time",
x = "Week",
y = "Positive Words Count",
caption = "Smoothed trend line"
) +
theme_dataedu() +
theme(
plot.title = element_text(hjust = 0.5),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
unlink("challenge1_cache", recursive = TRUE)
install.packages("tm")
install.packages("wordcloud")
install.packages("textdata")
library(tidyverse)
library(here)
library(tm)
library(dplyr)
library(wordcloud)
library(tidytext)
library(dataedu)
library(ggplot2)
file_path1 <- "week1.txt"
file_path2 <- "week2.txt"
file_path3 <- "week3.txt"
file_path4 <- "week4.txt"
file_path5 <- "week5.txt"
file_path6 <- "week6.txt"
text_lines1 <- readLines(file_path1, warn = FALSE)
text_lines2 <- readLines(file_path2, warn = FALSE)
text_lines3 <- readLines(file_path3, warn = FALSE)
text_lines4 <- readLines(file_path4, warn = FALSE)
text_lines5 <- readLines(file_path5, warn = FALSE)
text_lines6 <- readLines(file_path6, warn = FALSE)
week1_df <- data.frame(Content = text_lines1)
week2_df <- data.frame(Content = text_lines2)
week3_df <- data.frame(Content = text_lines3)
week4_df <- data.frame(Content = text_lines4)
week5_df <- data.frame(Content = text_lines5)
week6_df <- data.frame(Content = text_lines6)
week1Tokens <-
week1_df %>%
unnest_tokens(output = word, input = Content)
week2Tokens <-
week2_df %>%
unnest_tokens(output = word, input = Content)
week3Tokens <-
week3_df %>%
unnest_tokens(output = word, input = Content)
week4Tokens <-
week4_df %>%
unnest_tokens(output = word, input = Content)
week5Tokens <-
week5_df %>%
unnest_tokens(output = word, input = Content)
week6Tokens <-
week6_df %>%
unnest_tokens(output = word, input = Content)
data(stop_words)
week1Tokens <-
week1Tokens %>%
anti_join(stop_words, by = "word")
week2Tokens <-
week2Tokens %>%
anti_join(stop_words, by = "word")
week3Tokens <-
week3Tokens %>%
anti_join(stop_words, by = "word")
week4Tokens <-
week4Tokens %>%
anti_join(stop_words, by = "word")
week5Tokens <-
week5Tokens %>%
anti_join(stop_words, by = "word")
week6Tokens <-
week6Tokens %>%
anti_join(stop_words, by = "word")
combined_tokens <- bind_rows(week1Tokens, week2Tokens, week3Tokens, week4Tokens, week5Tokens, week6Tokens)
create_word_cloud <- function(tokens){
wordcloud(words = week1Tokens, min.freq = 1, scale = c(3, 0.5), colors = brewer.pal(8, "Dark2"))
}
word_cloud <- lapply(paste0("week",1:6,"Tokens"), create_word_cloud)
combined_tokens %>%
count(word, sort = TRUE)
most_occuring <- combined_tokens %>%
count(word, sort = TRUE) %>%
# n as a percent of total words
mutate(percent = n / sum(n) * 100)
get_sentiments("nrc")
# Only positive in the NRC dataset
nrc_pos <-
get_sentiments("nrc") %>%
filter(sentiment == "positive")
# Match to tokens
pos_tokens_count <-
combined_tokens %>%
inner_join(nrc_pos, by = "word") %>%
# Total appearance of positive words
count(word, sort = TRUE)
pos_tokens_count
pos_tokens_count %>%
# only words that appear 75 times or more
filter(n >= 3) %>%
ggplot(., aes(x = reorder(word, -n), y = n)) +
geom_bar(stat = "identity", fill = dataedu_colors("darkblue")) +
labs(
title = "Count of Words Associated with Positivity",
subtitle = "Words associated with postivity",
caption = "Data: Journal and NRC",
x = "",
y = "Count"
) +
theme_dataedu()
read_and_preprocess <- function(file_path) {
text <- tolower(readLines(file_path, warn = FALSE))
text <- gsub("[[:punct:]]", " ", text)
return(text)
}
texts <- lapply(paste0("week", 1:6, ".txt"), read_and_preprocess)
positive_words <- get_sentiments("afinn")$word
# Function to count positive words in a text
count_positive_words <- function(text) {
words <- unlist(strsplit(text, " "))
positive_count <- sum(words %in% positive_words)
return(positive_count)
}
positive_counts <- sapply(texts, count_positive_words)
weeks <- paste0("Week", 1:6)
df2 <- data.frame(Week = weeks, PositiveWordsCount = positive_counts)
ggplot(df, aes(x = Week, y = PositiveWordsCount)) +
geom_line(color = "blue",size = 2) +
geom_point(color = "red", size = 3) +
geom_smooth(method = "loess", se = FALSE, color = "green") +
labs(
title = "Count of Positive Words Over Time",
x = "Week",
y = "Positive Words Count",
caption = "Smoothed trend line"
) +
theme_dataedu() +
theme(
plot.title = element_text(hjust = 0.5),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
install.packages("tm")
install.packages("wordcloud")
install.packages("textdata")
library(tidyverse)
library(here)
library(tm)
library(dplyr)
library(wordcloud)
library(tidytext)
library(dataedu)
library(ggplot2)
file_path1 <- "week1.txt"
file_path2 <- "week2.txt"
file_path3 <- "week3.txt"
file_path4 <- "week4.txt"
file_path5 <- "week5.txt"
file_path6 <- "week6.txt"
text_lines1 <- readLines(file_path1, warn = FALSE)
text_lines2 <- readLines(file_path2, warn = FALSE)
text_lines3 <- readLines(file_path3, warn = FALSE)
text_lines4 <- readLines(file_path4, warn = FALSE)
text_lines5 <- readLines(file_path5, warn = FALSE)
text_lines6 <- readLines(file_path6, warn = FALSE)
week1_df <- data.frame(Content = text_lines1)
week2_df <- data.frame(Content = text_lines2)
week3_df <- data.frame(Content = text_lines3)
week4_df <- data.frame(Content = text_lines4)
week5_df <- data.frame(Content = text_lines5)
week6_df <- data.frame(Content = text_lines6)
week1Tokens <-
week1_df %>%
unnest_tokens(output = word, input = Content)
week2Tokens <-
week2_df %>%
unnest_tokens(output = word, input = Content)
week3Tokens <-
week3_df %>%
unnest_tokens(output = word, input = Content)
week4Tokens <-
week4_df %>%
unnest_tokens(output = word, input = Content)
week5Tokens <-
week5_df %>%
unnest_tokens(output = word, input = Content)
week6Tokens <-
week6_df %>%
unnest_tokens(output = word, input = Content)
data(stop_words)
week1Tokens <-
week1Tokens %>%
anti_join(stop_words, by = "word")
week2Tokens <-
week2Tokens %>%
anti_join(stop_words, by = "word")
week3Tokens <-
week3Tokens %>%
anti_join(stop_words, by = "word")
week4Tokens <-
week4Tokens %>%
anti_join(stop_words, by = "word")
week5Tokens <-
week5Tokens %>%
anti_join(stop_words, by = "word")
week6Tokens <-
week6Tokens %>%
anti_join(stop_words, by = "word")
combined_tokens <- bind_rows(week1Tokens, week2Tokens, week3Tokens, week4Tokens, week5Tokens, week6Tokens)
create_word_cloud <- function(tokens){
wordcloud(words = week1Tokens, min.freq = 1, scale = c(3, 0.5), colors = brewer.pal(8, "Dark2"))
}
word_cloud <- lapply(paste0("week",1:6,"Tokens"), create_word_cloud)
combined_tokens %>%
count(word, sort = TRUE)
most_occuring <- combined_tokens %>%
count(word, sort = TRUE) %>%
# n as a percent of total words
mutate(percent = n / sum(n) * 100)
get_sentiments("nrc")
# Only positive in the NRC dataset
nrc_pos <-
get_sentiments("nrc") %>%
filter(sentiment == "positive")
# Match to tokens
pos_tokens_count <-
combined_tokens %>%
inner_join(nrc_pos, by = "word") %>%
# Total appearance of positive words
count(word, sort = TRUE)
pos_tokens_count
pos_tokens_count %>%
# only words that appear 75 times or more
filter(n >= 3) %>%
ggplot(., aes(x = reorder(word, -n), y = n)) +
geom_bar(stat = "identity", fill = dataedu_colors("darkblue")) +
labs(
title = "Count of Words Associated with Positivity",
subtitle = "Words associated with postivity",
caption = "Data: Journal and NRC",
x = "",
y = "Count"
) +
theme_dataedu()
read_and_preprocess <- function(file_path) {
text <- tolower(readLines(file_path, warn = FALSE))
text <- gsub("[[:punct:]]", " ", text)
return(text)
}
texts <- lapply(paste0("week", 1:6, ".txt"), read_and_preprocess)
positive_words <- get_sentiments("afinn")$word
# Function to count positive words in a text
count_positive_words <- function(text) {
words <- unlist(strsplit(text, " "))
positive_count <- sum(words %in% positive_words)
return(positive_count)
}
positive_counts <- sapply(texts, count_positive_words)
weeks <- paste0("Week", 1:6)
df2 <- data.frame(Week = weeks, PositiveWordsCount = positive_counts)
ggplot(df, aes(x = Week, y = PositiveWordsCount)) +
geom_line(color = "blue",size = 2) +
geom_point(color = "red", size = 3) +
geom_smooth(method = "loess", se = FALSE, color = "green") +
labs(
title = "Count of Positive Words Over Time",
x = "Week",
y = "Positive Words Count",
caption = "Smoothed trend line"
) +
theme_dataedu() +
theme(
plot.title = element_text(hjust = 0.5),
axis.title = element_text(face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
View(df)
View(df2)
View(pos_tokens_count)
