
# Challenge 2 social Network Analysis

Like the steps done in the Text Analysis challenge, we begin here by installing the required packages and loading them.

```{r}

install.packages("tm")
install.packages("wordcloud")
install.packages("textdata")
install.packages("igraph")

library(tidyverse)
library(here)
library(tm)
library(dplyr)
library(tidytext)
library(igraph)
```

# importing the data

After the installation and loading of the library, the data will again be imported from the text files and transformed into data frames.

```{r}
file_path1 <- "week1.txt"
file_path2 <- "week2.txt"
file_path3 <- "week3.txt"
file_path4 <- "week4.txt"
file_path5 <- "week5.txt"
file_path6 <- "week6.txt"

text_lines1 <- readLines(file_path1, warn = FALSE)
text_lines2 <- readLines(file_path2, warn = FALSE)
text_lines3 <- readLines(file_path3, warn = FALSE)
text_lines4 <- readLines(file_path4, warn = FALSE)
text_lines5 <- readLines(file_path5, warn = FALSE)
text_lines6 <- readLines(file_path6, warn = FALSE)

week1_df <- data.frame(Content = text_lines1)
week2_df <- data.frame(Content = text_lines2)
week3_df <- data.frame(Content = text_lines3)
week4_df <- data.frame(Content = text_lines4)
week5_df <- data.frame(Content = text_lines5)
week6_df <- data.frame(Content = text_lines6)


```

# Tokenization

From the sentences, tokens(Words) will be created using the unnest_tokens functions.

```{r}
week1Tokens <- 
  week1_df %>%
  unnest_tokens(output = word, input = Content)

week2Tokens <- 
  week2_df %>%
  unnest_tokens(output = word, input = Content)

week3Tokens <- 
  week3_df %>%
  unnest_tokens(output = word, input = Content)

week4Tokens <- 
  week4_df %>%
  unnest_tokens(output = word, input = Content)

week5Tokens <- 
  week5_df %>%
  unnest_tokens(output = word, input = Content)

week6Tokens <- 
  week6_df %>%
  unnest_tokens(output = word, input = Content)

```

#Stop Word removal

To ensure that the words we use in the analysing step are the meanigful ones, we remove stops that do not necessary add or distort the context.

```{r}

data(stop_words)

week1Tokens <-
  week1Tokens %>%
  anti_join(stop_words, by = "word")

week2Tokens <-
  week2Tokens %>%
  anti_join(stop_words, by = "word")

week3Tokens <-
  week3Tokens %>%
  anti_join(stop_words, by = "word")

week4Tokens <-
  week4Tokens %>%
  anti_join(stop_words, by = "word")

week5Tokens <-
  week5Tokens %>%
  anti_join(stop_words, by = "word")

week6Tokens <-
  week6Tokens %>%
  anti_join(stop_words, by = "word")


```

All tokens will be combined in this step.

```{r}
combined_tokens <- bind_rows(week1Tokens, week2Tokens, week3Tokens, week4Tokens, week5Tokens, week6Tokens)

```

Now, the most occurring words will be filtered out and also the counts. This will be saved in a data frame freq_words.

```{r}
freq_words <-  combined_tokens %>% 
    count(word, sort = TRUE) 
freq_words <- freq_words[1:20, ]
```

# Visualization

# Concept Map

Using the Igraph library that was installed in an earlier step, a concept map will be created. In this map, concepts occurring the same number of times are joined by vertices.

```{r}


graph <- graph.data.frame(freq_words, directed = TRUE)


# Plot the graph
plot(graph, main = "Concept Map", vertex.label.cex = 1.0, edge.arrow.size = 0.5)


```
